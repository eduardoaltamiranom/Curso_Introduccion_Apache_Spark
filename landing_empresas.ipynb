{"nbformat_minor": 2, "cells": [{"source": "## 1\u00b0 PASO: Importamos m\u00f3dulos de apache spark", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *", "outputs": [], "metadata": {}}, {"source": "## 2\u00b0 PASO: Creamos las session de apache spark en una variable", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()", "outputs": [], "metadata": {}}, {"source": "## 3\u00b0 PASO: Verificamos la versi\u00f3n de apache spark", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "spark", "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "<pyspark.sql.session.SparkSession at 0x7f03b05a2750>", "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-81f7-m.us-central1-b.c.weighty-works-292723.internal:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "}, "metadata": {}}], "metadata": {}}, {"source": "## 4\u00b0 PASO: Crear la estructura del dataframe", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "df_schema = StructType([\nStructField(\"ID\", StringType(),True),\nStructField(\"EMPRESA_NAME\", StringType(),True)\n])", "outputs": [], "metadata": {}}, {"source": "## 5\u00b0 Definimos ruta del archivo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "ruta_lectura = \"hdfs:/datalake/workload/empresa.data\"", "outputs": [], "metadata": {}}, {"source": "## 6\u00b0 Creamos el dataframe de Persona", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "df_with_schema = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(df_schema).load(ruta_lectura)", "outputs": [], "metadata": {}}, {"source": "## 7\u00b0 Mostramos el dataframe cargado en memoria", "cell_type": "markdown", "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "df_with_schema.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+------------+\n| ID|EMPRESA_NAME|\n+---+------------+\n|  1|     Walmart|\n|  2|   Microsoft|\n|  3|       Apple|\n|  4|      Toyota|\n|  5|      Amazon|\n|  6|      Google|\n|  7|     Samsung|\n|  8|          HP|\n|  9|         IBM|\n| 10|        Sony|\n+---+------------+\n\n"}], "metadata": {}}, {"source": "## 8\u00b0 Definimos la ruta en hdfs donde almacenaremos el archivo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "ruta_destino = \"hdfs:/datalake/landing/empresas/empresa.parquet\"", "outputs": [], "metadata": {}}, {"source": "## 9\u00b0 Guardamos el archivo en formato parquet", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "df_with_schema.repartition(1).write.mode(\"overwrite\").format(\"parquet\").save(ruta_destino)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}